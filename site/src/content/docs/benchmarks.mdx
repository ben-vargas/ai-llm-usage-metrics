---
title: Benchmarks
description: Production benchmark comparison of llm-usage-metrics and ccusage-codex.
---

## Production benchmark (February 24, 2026)

This benchmark compares:

- `ccusage-codex monthly`
- `llm-usage monthly --provider openai` (the installed binary for `llm-usage-metrics`)

Both commands were executed on the same machine, against real local production data, with repeated timed runs.

You can reproduce this benchmark with the built-in script:

```bash
pnpm run perf:production-benchmark -- --runs 5
```

To export reusable artifacts:

```bash
pnpm run perf:production-benchmark -- \
  --runs 5 \
  --json-output ./tmp/production-benchmark.json \
  --markdown-output ./tmp/production-benchmark.md
```

## Baseline machine

| Spec | Value |
| --- | --- |
| OS | CachyOS (Linux 6.19.2-2-cachyos) |
| CPU | Intel Core Ultra 9 185H (22 logical CPUs, up to 5.10 GHz) |
| Memory | 62 GiB RAM + 62 GiB swap |
| Storage | NVMe SSD (KXG8AZNV1T02 LA KIOXIA, 953.9 GiB) |
| Node.js | v24.12.0 |
| pnpm | 10.17.1 |
| `ccusage-codex` | 18.0.5 |
| `llm-usage` (`llm-usage-metrics`) | 0.3.2 |

## Cache modes used

- `no cache`
  - fresh `XDG_CACHE_HOME` for each run
  - `ccusage-codex`: `--no-offline`
  - `llm-usage`: `LLM_USAGE_PARSE_CACHE_ENABLED=0` and no `--pricing-offline`
- `with cache`
  - dedicated warm cache directory
  - `ccusage-codex`: `--offline`
  - `llm-usage`: `--pricing-offline` with warmed parse cache

For repeatability, `LLM_USAGE_SKIP_UPDATE_CHECK=1` was set for `llm-usage` benchmark runs.

## Commands benchmarked

```bash
# ccusage-codex
ccusage-codex monthly --json
ccusage-codex monthly --offline --json

# llm-usage-metrics (installed as llm-usage)
llm-usage monthly --provider openai --json
llm-usage monthly --provider openai --pricing-offline --json
```

## Runtime results (5 runs each)

| Tool | Cache mode | Median (s) | Mean (s) | Min (s) | Max (s) |
| --- | --- | ---: | ---: | ---: | ---: |
| `ccusage-codex monthly` | no cache | 14.247 | 14.456 | 14.189 | 15.166 |
| `ccusage-codex monthly --offline` | with cache | 14.043 | 14.268 | 13.917 | 14.923 |
| `llm-usage monthly --provider openai` | no cache | 4.192 | 4.196 | 4.101 | 4.270 |
| `llm-usage monthly --provider openai --pricing-offline` | with cache | 0.793 | 0.784 | 0.729 | 0.840 |

Derived from median runtime:

- `llm-usage` vs `ccusage-codex` (no cache): `3.40x` faster
- `llm-usage` vs `ccusage-codex` (with cache): `17.71x` faster
- `llm-usage` cache effect: `5.29x` faster with cache
- `ccusage-codex` cache effect: `1.01x` faster with cache

## Dataset scope observed during benchmark

These commands do not cover identical scope, so compare runtime with that context.

| Tool | Scope snapshot from benchmark run |
| --- | --- |
| `llm-usage monthly --provider openai` | 559 session files, 81,275 parsed events (`pi`: 174 files / 18,627 events, `codex`: 384 / 58,366, `opencode`: 1 / 4,282) |
| `ccusage-codex monthly` | Codex-only report (`monthly` array: 3 periods, plus `totals`) |

## Interpretation

- `llm-usage` benefits strongly from parse + pricing cache in repeated runs.
- `ccusage-codex` runtime is similar in both modes on this machine and dataset.
- Results are production-real for this machine and data, not universal constants. Re-run on your own workload before making broad conclusions.
